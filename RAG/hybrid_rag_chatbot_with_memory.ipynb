{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hybrid RAG Chatbot: LangChain + Direct OpenAI API with Memory\n",
        "\n",
        "## Learning Objectives\n",
        "In this notebook, you'll learn:\n",
        "1. **LangChain for Retrieval**: Use LangChain for web scraping, document processing, embeddings, and similarity search\n",
        "2. **Direct OpenAI API**: Use OpenAI's `responses.create()` endpoint directly for more control\n",
        "3. **Conversation Memory**: Implement custom conversation memory without LangChain's memory classes\n",
        "4. **Hybrid Architecture**: Understand when to use each tool and how to integrate them\n",
        "\n",
        "## Architecture Overview\n",
        "```\n",
        "User Question\n",
        "     ‚Üì\n",
        "[LangChain] ‚Üí Similarity Search ‚Üí Retrieved Documents\n",
        "     ‚Üì\n",
        "[OpenAI API] ‚Üí responses.create() with Memory ‚Üí Answer\n",
        "     ‚Üì\n",
        "Update Conversation History\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 1: Setup and Installation\n",
        "Install all required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -qU langchain langchain-core langchain-openai langchain-community chromadb beautifulsoup4 openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: Initialize OpenAI Client\n",
        "\n",
        "**Important Security Note**: Never hardcode API keys in production code.\n",
        "Use environment variables or secure vaults instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ OpenAI client initialized successfully\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# SECURITY WARNING: Use environment variables for API keys\n",
        "# Load your API key from .env file: OPENAI_API_KEY=your_key_here\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')  # Set this in your .env file\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "# This client will be used for direct API calls later\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "print(\"‚úÖ OpenAI client initialized successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 3: Data Collection with LangChain\n",
        "\n",
        "### Step 3.1: Define URLs to Scrape\n",
        "We'll use Wikipedia's Taj Mahal article as our knowledge source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìö Collected 1 URL(s) to process\n",
            "URLs: ['https://en.wikipedia.org/wiki/Taj_Mahal']\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Define the URL(s) to scrape\n",
        "urls = ['https://en.wikipedia.org/wiki/Taj_Mahal']\n",
        "\n",
        "print(f\"üìö Collected {len(urls)} URL(s) to process\")\n",
        "print(f\"URLs: {urls}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3.2: Load Documents with LangChain\n",
        "\n",
        "**What is WebBaseLoader?**\n",
        "- A LangChain utility that fetches web pages and converts them to Document objects\n",
        "- Handles HTTP requests and parsing automatically\n",
        "- Returns structured data that's easy to process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded 1 document(s)\n",
            "Document type: <class 'langchain_core.documents.base.Document'>\n",
            "\n",
            "First 500 characters:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Taj Mahal - Wikipedia\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Jump to content\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Main menu\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Main menu\n",
            "move to sidebar\n",
            "hide\n",
            "\n",
            "\n",
            "\n",
            "\t\tNavigation\n",
            "\t\n",
            "\n",
            "\n",
            "Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\t\tContribute\n",
            "\t\n",
            "\n",
            "\n",
            "HelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Search\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Search\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Appearance\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Donate\n",
            "\n",
            "Create account\n",
            "\n",
            "Log in\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Personal tools\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Donate Create account Log in\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Create a loader with custom headers to avoid being blocked\n",
        "loader = WebBaseLoader(\n",
        "    urls,\n",
        "    requests_kwargs={\n",
        "        'headers': {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36'\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "# Load the documents\n",
        "pages = loader.load()\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(pages)} document(s)\")\n",
        "print(f\"Document type: {type(pages[0])}\")\n",
        "print(f\"\\nFirst 500 characters:\\n{pages[0].page_content[:500]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3.3: Document Statistics\n",
        "Let's analyze the scraped data before processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Document Statistics:\n",
            "Total pages: 1\n",
            "Total characters: 66,994\n",
            "Average chars/page: 66,994.0\n",
            "Min/Max chars: 66,994 / 66,994\n"
          ]
        }
      ],
      "source": [
        "# Calculate character counts for each page\n",
        "char_counts = [len(d.page_content or \"\") for d in pages]\n",
        "\n",
        "print(\"üìä Document Statistics:\")\n",
        "print(f\"Total pages: {len(pages)}\")\n",
        "print(f\"Total characters: {sum(char_counts):,}\")\n",
        "print(f\"Average chars/page: {round(sum(char_counts) / max(1, len(char_counts)), 2):,}\")\n",
        "print(f\"Min/Max chars: {min(char_counts, default=0):,} / {max(char_counts, default=0):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 4: Data Preprocessing\n",
        "\n",
        "### Step 4.1: Normalize Text\n",
        "\n",
        "**Why normalize?**\n",
        "- Web-scraped text often contains irregular whitespace, HTML entities, and special characters\n",
        "- Normalization improves embedding quality and retrieval accuracy\n",
        "- Reduces noise in the vector database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Documents normalized\n",
            "\n",
            "Cleaned text preview (first 500 chars):\n",
            "Taj Mahal - Wikipedia\n",
            "\n",
            "Jump to content\n",
            "\n",
            "Main menu\n",
            "\n",
            "Main menu\n",
            "move to sidebar\n",
            "hide\n",
            "\n",
            "Navigation\n",
            "\n",
            "\n",
            "Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us\n",
            "\n",
            "Contribute\n",
            "\n",
            "\n",
            "HelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\n",
            "\n",
            "Search\n",
            "\n",
            "Search\n",
            "\n",
            "Appearance\n",
            "\n",
            "Donate\n",
            "\n",
            "Create account\n",
            "\n",
            "Log in\n",
            "\n",
            "Personal tools\n",
            "\n",
            "Donate Create account Log in\n",
            "\n",
            "Contents\n",
            "move to sidebar\n",
            "hide\n",
            "\n",
            "(Top)\n",
            "\n",
            "1\n",
            "Etymology\n",
            "\n",
            "2\n",
            "Inspiration\n",
            "\n",
            "3\n",
            "Architecture and design\n",
            "\n",
            "Toggle Architecture and design subsection\n",
            "\n",
            "3.1\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import html\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def normalize_doc(doc: Document) -> Document:\n",
        "    \"\"\"\n",
        "    Normalize a document by cleaning up whitespace and special characters.\n",
        "    \n",
        "    Steps:\n",
        "    1. Unescape HTML entities (e.g., &nbsp; ‚Üí space)\n",
        "    2. Replace non-breaking spaces with regular spaces\n",
        "    3. Remove zero-width characters\n",
        "    4. Collapse multiple spaces/tabs into single space\n",
        "    5. Limit consecutive newlines to maximum of 2\n",
        "    6. Trim whitespace around newlines\n",
        "    \n",
        "    Args:\n",
        "        doc: LangChain Document object\n",
        "    \n",
        "    Returns:\n",
        "        Cleaned Document object with same metadata\n",
        "    \"\"\"\n",
        "    text = doc.page_content or \"\"\n",
        "    \n",
        "    # Step 1: Unescape HTML entities\n",
        "    text = html.unescape(text)\n",
        "    \n",
        "    # Step 2: Replace non-breaking spaces\n",
        "    text = text.replace(\"\\xa0\", \" \")\n",
        "    \n",
        "    # Step 3: Remove zero-width characters\n",
        "    text = re.sub(r\"[\\u200B-\\u200D\\uFEFF]\", \"\", text)\n",
        "    \n",
        "    # Step 4: Collapse spaces and tabs\n",
        "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
        "    \n",
        "    # Step 5: Limit blank lines\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "    \n",
        "    # Step 6: Trim around newlines\n",
        "    text = re.sub(r\" *\\n *\", \"\\n\", text).strip()\n",
        "    \n",
        "    return Document(page_content=text, metadata=doc.metadata)\n",
        "\n",
        "# Apply normalization to all documents\n",
        "clean_docs = [normalize_doc(d) for d in pages]\n",
        "\n",
        "print(\"‚úÖ Documents normalized\")\n",
        "print(f\"\\nCleaned text preview (first 500 chars):\\n{clean_docs[0].page_content[:500]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4.2: Split Documents into Chunks\n",
        "\n",
        "**Why chunk documents?**\n",
        "- Embedding models have token limits (typically 8,192 tokens)\n",
        "- Smaller chunks = more precise retrieval\n",
        "- Overlap ensures context isn't lost at chunk boundaries\n",
        "\n",
        "**Parameters explained:**\n",
        "- `chunk_size=2000`: Each chunk will be ~2000 characters\n",
        "- `chunk_overlap=150`: 150 characters overlap between chunks to maintain context\n",
        "- `separators`: Try to split on paragraphs first, then lines, then words, then characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Created 50 chunks from 1 document(s)\n",
            "\n",
            "Example chunk:\n",
            "--------------------------------------------------------------------------------\n",
            "Taj Mahal - Wikipedia\n",
            "\n",
            "Jump to content\n",
            "\n",
            "Main menu\n",
            "\n",
            "Main menu\n",
            "move to sidebar\n",
            "hide\n",
            "\n",
            "Navigation\n",
            "\n",
            "\n",
            "Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us\n",
            "\n",
            "Contribute\n",
            "\n",
            "\n",
            "HelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\n",
            "\n",
            "Search\n",
            "\n",
            "Search\n",
            "\n",
            "Appearance\n",
            "\n",
            "Donate\n",
            "\n",
            "Create account\n",
            "\n",
            "Log in\n",
            "\n",
            "Personal tools\n",
            "\n",
            "Donate Create account Log in\n",
            "\n",
            "Contents\n",
            "move to sidebar\n",
            "hide\n",
            "\n",
            "(Top)\n",
            "\n",
            "1\n",
            "Etymology\n",
            "\n",
            "2\n",
            "Inspiration\n",
            "\n",
            "3\n",
            "Architecture and design\n",
            "\n",
            "Toggle Architecture and design subsection\n",
            "\n",
            "3.1\n",
            "Exterior\n",
            "\n",
            "3.2\n",
            "Interior\n",
            "\n",
            "3.3\n",
            "Garden\n",
            "\n",
            "3.4\n",
            "Outlying buildings\n",
            "\n",
            "4\n",
            "Construction\n",
            "\n",
            "5\n",
            "Later years\n",
            "\n",
            "6\n",
            "Symbolism\n",
            "\n",
            "7\n",
            "Tourism\n",
            "\n",
            "8\n",
            "Myths\n",
            "\n",
            "9\n",
            "See also\n",
            "\n",
            "10\n",
            "Notes\n",
            "\n",
            "11\n",
            "References\n",
            "\n",
            "Toggle References subsection\n",
            "\n",
            "11.1\n",
            "Citations\n",
            "\n",
            "11.2\n",
            "General sources\n",
            "\n",
            "12\n",
            "External links\n",
            "\n",
            "Toggle the table of contents\n",
            "\n",
            "Taj Mahal\n",
            "\n",
            "168 languages\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Create a text splitter with sensible defaults\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2000,           # Maximum characters per chunk\n",
        "    chunk_overlap=150,          # Overlap to maintain context\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # Try these separators in order\n",
        ")\n",
        "\n",
        "# Split documents into chunks\n",
        "chunks = splitter.split_documents(clean_docs)\n",
        "\n",
        "print(f\"‚úÖ Created {len(chunks)} chunks from {len(clean_docs)} document(s)\")\n",
        "print(f\"\\nExample chunk:\\n{'-' * 80}\\n{chunks[0].page_content}\\n{'-' * 80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 5: Create Vector Database with LangChain\n",
        "\n",
        "### Step 5.1: Generate Embeddings and Store in ChromaDB\n",
        "\n",
        "**What are embeddings?**\n",
        "- Numerical representations of text that capture semantic meaning\n",
        "- Similar text ‚Üí similar embedding vectors\n",
        "- Enables semantic search (find by meaning, not just keywords)\n",
        "\n",
        "**What is ChromaDB?**\n",
        "- Open-source vector database\n",
        "- Stores embeddings and enables fast similarity search\n",
        "- Runs locally without external dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Creating embeddings and building vector database...\n",
            "This may take a minute depending on the number of chunks...\n",
            "üìÅ Database will be saved to: ./chroma_db\n",
            "‚úÖ Vector database created with 50 chunks\n",
            "‚úÖ Database persisted to: ./chroma_db\n",
            "Collection name: taj_mahal_knowledge\n",
            "‚úÖ Directory created successfully\n",
            "üìä Directory size: 1280.00 KB\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "import os\n",
        "\n",
        "# Define the directory where vector database will be stored\n",
        "PERSIST_DIRECTORY = \"./chroma_db\"\n",
        "\n",
        "# Initialize OpenAI embeddings\n",
        "# This uses OpenAI's text-embedding-ada-002 model by default\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    openai_api_key=openai_api_key\n",
        ")\n",
        "\n",
        "print(\"üîÑ Creating embeddings and building vector database...\")\n",
        "print(\"This may take a minute depending on the number of chunks...\")\n",
        "print(f\"üìÅ Database will be saved to: {PERSIST_DIRECTORY}\")\n",
        "\n",
        "# Create vector database from documents with persistence\n",
        "# This will:\n",
        "# 1. Generate embeddings for each chunk using OpenAI\n",
        "# 2. Store embeddings in ChromaDB\n",
        "# 3. Save to local directory for future use\n",
        "# 4. Create an index for fast similarity search\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    collection_name=\"taj_mahal_knowledge\",\n",
        "    persist_directory=PERSIST_DIRECTORY  # üëà This saves to disk!\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Vector database created with {len(chunks)} chunks\")\n",
        "print(f\"‚úÖ Database persisted to: {PERSIST_DIRECTORY}\")\n",
        "print(f\"Collection name: taj_mahal_knowledge\")\n",
        "\n",
        "# Verify the directory was created\n",
        "if os.path.exists(PERSIST_DIRECTORY):\n",
        "    print(f\"‚úÖ Directory created successfully\")\n",
        "    print(f\"üìä Directory size: {sum(os.path.getsize(os.path.join(PERSIST_DIRECTORY, f)) for f in os.listdir(PERSIST_DIRECTORY) if os.path.isfile(os.path.join(PERSIST_DIRECTORY, f))) / 1024:.2f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5.2: Create a Retriever\n",
        "\n",
        "**What is a retriever?**\n",
        "- Interface for querying the vector database\n",
        "- Takes a question and returns most relevant chunks\n",
        "- Uses cosine similarity to find semantically similar content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Retriever created\n",
            "Configuration: Top 4 most relevant chunks will be retrieved\n"
          ]
        }
      ],
      "source": [
        "# Create a retriever from the vector database\n",
        "# Default: returns top 4 most similar chunks\n",
        "retriever = vectordb.as_retriever(\n",
        "    search_kwargs={\"k\": 4}  # Number of chunks to retrieve\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Retriever created\")\n",
        "print(\"Configuration: Top 4 most relevant chunks will be retrieved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5.3: Test the Retriever\n",
        "Let's verify that similarity search works correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Testing retrieval with question: 'What materials were used in the main mausoleum and where did some of these materials originate?'\n",
            "\n",
            "Found 4 relevant chunks:\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "üìÑ Document 1:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Exterior\n",
            "The eight sided main structure with large arched doorways and minarets on a square plinth\n",
            "The mausoleum building is the central structure of the entire complex. It is a white marble structure standing on a 6-metre (20 ft) high square plinth with sides measuring 95.5 metres (313 ft) in length. The base structure is a large multi-chambered cube with chamfered corners forming an eight-sided structure that is approximately 57.3 metres (188 ft) long on each of the four long sides.[24]\n",
            "The building has four identical sides with iwans (arch-shaped doorways), topped by a large dome and finial. Each side of the iwan is framed with a 33-metre (108 ft) high pishtaq (vaulted archway) with two similarly shaped arched balconies stacked on either side. This motif of archways is replicated on a smaller scale on the chamfered corner areas, making the design completely symmetrical.[25] At the southern side of the platform, facing the garden, there are two flights of stairs on either side which are partly covered and provide the only access from ground level up to the mausoleum building.[23]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "üìÑ Document 2:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "The large onion dome topped by a finialFinial, tamga of the Mughal Empire\n",
            "The predominant feature of the mausoleum is the 23-metre (75 ft) high marble dome that surmounts the tomb. The onion shaped dome sits on a 12-metre (39 ft) high cylindrical drum with an inner diameter of 18.4 metres (60 ft).[26] The dome is slightly asymmetrical and is topped by a 9.6-metre (31 ft) high gilded finial.[19][27] The intermediate zone between the drum and the dome is supplanted by an ornamental moulding with a twisted rope design.[23]\n",
            "The main dome is surrounded by four smaller domes or chattris placed at its corners, which replicate the onion shape of the main dome. The smaller domes are supported by columns which stand on the top of the main structure and help bring light to the interior of the building. Tall spires called guldastas extend from edges of walls which serve as decorative elements. The main and the smaller domes are decorated with a design resembling a lotus flower.[23] The domes are topped by decorative finials which uses Persian and Indian design elements.[28] The main finial was originally made of gold but was replaced by a copy made of gilded bronze in the early 19th century.[23] The finial is topped by a moon, a typical Islamic motif, whose horns point heavenward.[29]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "üìÑ Document 3:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "The interior walls are about 25 metres (82 ft) high and are topped by a \"false\" interior dome decorated with a sun motif. The inlay work is a lapidary of precious and semiprecious gemstones.[36] Each chamber wall is highly decorated with dado bas-relief, intricate lapidary inlay and refined calligraphy panels similar to the design elements seen throughout the exterior of the complex.[37] The main chamber houses the false sarcophagi of Mumtaz Mahal and Shah Jahan, while the real ones are in the basement.[25] Perforated marble jalis (mahjar-i mushabbak) border the cenotaphs and are made from eight marble panels carved through with intricate pierce work inlaid in delicate detail with semi-precious stones. The cenotaphs were originally covered by a screen made of gold on the occasion of the second anniversary of Mumtaz Mahal's death in 1633, which was later replaced by the marble screen in 1643.[35]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "üìÑ Document 4:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Interior\n",
            "The central chamber with the cenotaphsA jali screen surrounding the cenotaphs\n",
            "The main inner chamber is an octagon with 7.3-metre (24 ft) sides, with the design allowing for entry from each face with the main door facing the garden to the south. Two tiers of eight pishtaq arches are located along the walls, similar to the exterior.[34] The four central upper arches form balconies or viewing areas, and each balcony's exterior window has an intricate jali. The inner wall is open along the axes where jali screens are fitted which transmit light from the exterior to the interior of the main chamber.[35] Except the south side, other three sides consist of an open elongated room flanked by two square cells covered with decorated ceilings set on the platform. The central room has arched openings on three sides fitted with jalis filled with panes of glass and a small rectangular window cut into the central jali.[35] The square cells which are reached through separate doors were probably originally used for visitors and Qur'an reciters as a place to rest. Staircases lead from the ground floor to the roof level, where there are corridors between the central hall and the two corner rooms in the south with a system of ventilation shafts.[23]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "def pretty_print_docs(docs):\n",
        "    \"\"\"Helper function to display retrieved documents in a readable format\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 100)\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        print(f\"\\nüìÑ Document {i}:\")\n",
        "        print(\"-\" * 100)\n",
        "        print(doc.page_content)\n",
        "        print(\"-\" * 100)\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "# Test query\n",
        "test_question = \"What materials were used in the main mausoleum and where did some of these materials originate?\"\n",
        "\n",
        "print(f\"üîç Testing retrieval with question: '{test_question}'\\n\")\n",
        "\n",
        "# Retrieve relevant documents\n",
        "retrieved_docs = retriever.invoke(test_question)\n",
        "\n",
        "print(f\"Found {len(retrieved_docs)} relevant chunks:\")\n",
        "pretty_print_docs(retrieved_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 6: Conversation Memory Management\n",
        "\n",
        "### Understanding Conversation Memory\n",
        "\n",
        "**Why do we need memory?**\n",
        "- OpenAI's API is stateless (doesn't remember previous interactions)\n",
        "- We need to manually track conversation history\n",
        "- Memory enables context-aware conversations and follow-up questions\n",
        "\n",
        "**Our Memory Implementation:**\n",
        "1. **Storage**: List of dictionaries with `role` and `message` keys\n",
        "2. **Sliding Window**: Keep only last 10 messages to avoid token overflow\n",
        "3. **Formatting**: Convert history to readable text for the LLM\n",
        "4. **Commands**: Support for viewing and clearing history\n",
        "\n",
        "**Memory Functions:**\n",
        "- `add_to_memory()`: Saves user/assistant messages\n",
        "- `get_memory_text()`: Formats history as string\n",
        "- `clear_memory()`: Resets conversation state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Memory system ready!\n"
          ]
        }
      ],
      "source": [
        "# Initialize conversation memory as a list\n",
        "# Each entry is a dictionary with 'role' and 'message' keys\n",
        "conversation_history = []\n",
        "\n",
        "def add_to_memory(role, message):\n",
        "    \"\"\"\n",
        "    Save a message to conversation memory.\n",
        "    \n",
        "    Args:\n",
        "        role (str): Either 'user' or 'assistant'\n",
        "        message (str): The actual message content\n",
        "    \n",
        "    Implements sliding window: keeps only last 10 messages\n",
        "    to prevent context overflow and manage token limits.\n",
        "    \"\"\"\n",
        "    conversation_history.append({\"role\": role, \"message\": message})\n",
        "    \n",
        "    # Keep only last 10 messages (5 exchanges)\n",
        "    if len(conversation_history) > 10:\n",
        "        conversation_history.pop(0)  # Remove oldest message\n",
        "\n",
        "def get_memory_text():\n",
        "    \"\"\"\n",
        "    Format conversation history as readable text.\n",
        "    \n",
        "    Returns:\n",
        "        str: Formatted conversation history or default message\n",
        "    \n",
        "    Format: \"user: question\\nassistant: answer\\n...\"\n",
        "    This gets included in the instructions to provide context.\n",
        "    \"\"\"\n",
        "    if not conversation_history:\n",
        "        return \"No previous conversation.\"\n",
        "    \n",
        "    return \"\\n\".join([f\"{msg['role']}: {msg['message']}\" for msg in conversation_history])\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"\n",
        "    Clear all conversation memory.\n",
        "    Useful for starting fresh or when context becomes confusing.\n",
        "    \"\"\"\n",
        "    conversation_history.clear()\n",
        "    print(\"üßπ Memory cleared!\")\n",
        "\n",
        "print(\"‚úÖ Memory system initialized!\")\n",
        "print(f\"üìä Current memory size: {len(conversation_history)} messages\")\n",
        "print(\"üíæ Max capacity: 10 messages (sliding window)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 7: Direct OpenAI API Integration\n",
        "\n",
        "### Understanding `responses.create()` Endpoint\n",
        "\n",
        "**Key Differences from `chat.completions.create()`:**\n",
        "\n",
        "| Feature | `responses.create()` | `chat.completions.create()` |\n",
        "|---------|---------------------|----------------------------|\n",
        "| **Parameters** | `input` + `instructions` | `messages` array |\n",
        "| **Structure** | Simpler, two-parameter model | Structured message history |\n",
        "| **System Prompt** | `instructions` parameter | First message with role=\"system\" |\n",
        "| **User Input** | `input` parameter | Messages with role=\"user\" |\n",
        "| **Memory Handling** | Manual formatting required | Array structure |\n",
        "| **Use Case** | Simple Q&A, single responses | Multi-turn conversations |\n",
        "\n",
        "**Why use `responses.create()` here?**\n",
        "- Cleaner separation of context (retrieved docs) and conversation history\n",
        "- Easier to template and format different types of input\n",
        "- More explicit control over what goes where\n",
        "\n",
        "**Our Implementation Strategy:**\n",
        "1. Retrieve relevant documents from vector database\n",
        "2. Get conversation history from memory\n",
        "3. Format `instructions` with memory + system prompt\n",
        "4. Format `input` with retrieved context + current question\n",
        "5. Get response and extract clean text\n",
        "6. Update memory with Q&A pair"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 8: Interactive Chatbot Interface\n",
        "\n",
        "### Chatbot Features\n",
        "\n",
        "**Commands:**\n",
        "- **Regular input**: Ask any question about the Taj Mahal\n",
        "- **`history`**: View entire conversation history\n",
        "- **`clear`**: Reset conversation memory\n",
        "- **`quit`** or **`exit`**: End the session\n",
        "\n",
        "**How it works:**\n",
        "1. User enters a question\n",
        "2. System processes special commands (history/clear/quit)\n",
        "3. For regular questions:\n",
        "   - Retrieves relevant context from vector database\n",
        "   - Includes conversation memory for context\n",
        "   - Generates answer using OpenAI API\n",
        "   - Saves Q&A to memory\n",
        "4. Displays formatted response\n",
        "\n",
        "**Example Conversation Flow:**\n",
        "```\n",
        "User: \"Who built the Taj Mahal?\"\n",
        "Bot: \"Shah Jahan built the Taj Mahal...\"\n",
        "\n",
        "User: \"Why did he build it?\"  ‚Üê Note: \"he\" requires memory!\n",
        "Bot: \"He built it in memory of his wife Mumtaz Mahal...\"\n",
        "```\n",
        "\n",
        "The chatbot understands pronouns and references because it maintains conversation memory!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Chat function ready!\n"
          ]
        }
      ],
      "source": [
        "def ask_question(question):\n",
        "    \"\"\"\n",
        "    Process a question using RAG (Retrieval-Augmented Generation) with memory.\n",
        "    \n",
        "    This function implements the complete RAG pipeline:\n",
        "    1. RETRIEVE: Get relevant context from vector database\n",
        "    2. AUGMENT: Combine context with conversation memory\n",
        "    3. GENERATE: Use OpenAI API to create response\n",
        "    4. REMEMBER: Save interaction to memory\n",
        "    \n",
        "    Args:\n",
        "        question (str): User's question\n",
        "    \n",
        "    Returns:\n",
        "        str: AI-generated answer based on retrieved context and memory\n",
        "    \n",
        "    Architecture:\n",
        "        Question ‚Üí Retriever ‚Üí Context\n",
        "                              ‚Üì\n",
        "        Memory ‚Üí Format ‚Üí Instructions\n",
        "                              ‚Üì\n",
        "        Context + Question ‚Üí Input\n",
        "                              ‚Üì\n",
        "        OpenAI API ‚Üí Response ‚Üí Update Memory\n",
        "    \"\"\"\n",
        "    \n",
        "    # ============================================================\n",
        "    # STEP 1: RETRIEVE - Find relevant documents from vector DB\n",
        "    # ============================================================\n",
        "    # Use LangChain retriever to find top 4 most similar chunks\n",
        "    docs = retriever.invoke(question)\n",
        "    \n",
        "    # Combine all retrieved documents into single context string\n",
        "    # Separated by double newlines for readability\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    \n",
        "    # ============================================================\n",
        "    # STEP 2: GET MEMORY - Retrieve conversation history\n",
        "    # ============================================================\n",
        "    # Format previous Q&A pairs as text to maintain context\n",
        "    memory = get_memory_text()\n",
        "    \n",
        "    # ============================================================\n",
        "    # STEP 3: BUILD INSTRUCTIONS - System prompt + memory\n",
        "    # ============================================================\n",
        "    # This acts as the \"system message\" in traditional chat models\n",
        "    # Includes:\n",
        "    # - Role definition (helpful assistant about Taj Mahal)\n",
        "    # - Previous conversation for context\n",
        "    # - Guidelines for using both memory and retrieved context\n",
        "    instructions = f\"\"\"You are a helpful assistant about the Taj Mahal.\n",
        "\n",
        "Previous conversation:\n",
        "{memory}\n",
        "\n",
        "Use the conversation history and context to answer questions accurately.\n",
        "If the user refers to previous topics (like \"it\", \"that\", \"he/she\"), \n",
        "use the conversation history to understand what they're referring to.\"\"\"\n",
        "    \n",
        "    # ============================================================\n",
        "    # STEP 4: BUILD INPUT - Combine retrieved context + question\n",
        "    # ============================================================\n",
        "    # This is the \"user message\" containing:\n",
        "    # - Retrieved context from vector database (RAG component)\n",
        "    # - Current question from user\n",
        "    input_text = f\"\"\"Context from knowledge base:\n",
        "{context}\n",
        "\n",
        "Current question: {question}\"\"\"\n",
        "    \n",
        "    # ============================================================\n",
        "    # STEP 5: CALL OPENAI RESPONSES API\n",
        "    # ============================================================\n",
        "    # Using responses.create() instead of chat.completions.create()\n",
        "    # Parameters:\n",
        "    # - model: gpt-4o-mini (fast and cost-effective)\n",
        "    # - input: User query + retrieved context\n",
        "    # - instructions: System prompt + conversation memory\n",
        "    # - temperature: 0.7 (balanced creativity/accuracy)\n",
        "    # - max_output_tokens: 500 (limit response length)\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        input=input_text,\n",
        "        instructions=instructions,\n",
        "        temperature=0.7,\n",
        "        max_output_tokens=500\n",
        "    )\n",
        "    \n",
        "    # ============================================================\n",
        "    # STEP 6: EXTRACT CLEAN TEXT\n",
        "    # ============================================================\n",
        "    # response.output_text provides direct string output\n",
        "    # No need to parse complex response objects\n",
        "    answer = response.output_text\n",
        "    \n",
        "    # ============================================================\n",
        "    # STEP 7: SAVE TO MEMORY - Remember this interaction\n",
        "    # ============================================================\n",
        "    # Add both question and answer to conversation history\n",
        "    # This enables follow-up questions and contextual understanding\n",
        "    add_to_memory(\"user\", question)\n",
        "    add_to_memory(\"assistant\", answer)\n",
        "    \n",
        "    return answer\n",
        "\n",
        "print(\"‚úÖ RAG + Memory chat function ready!\")\n",
        "print(\"üîç Retrieval: LangChain vector database\")\n",
        "print(\"ü§ñ Generation: OpenAI responses.create()\")\n",
        "print(\"üíæ Memory: Custom conversation tracking\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üéâ TAJ MAHAL CHATBOT\n",
            "================================================================================\n",
            "Ask me anything about the Taj Mahal!\n",
            "Commands: quit | clear | history\n",
            "================================================================================\n",
            "\n",
            "\n",
            "ü§ñ Bot: The Taj Mahal was commissioned by Shah Jahan, the fifth Mughal emperor of India, in 1631. It was built as a mausoleum for his beloved wife, Mumtaz Mahal, and also houses Shah Jahan's tomb. The construction was overseen by a board of architects led by Ustad Ahmad Lahori.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "ü§ñ Bot: The Taj Mahal is special for several reasons:\n",
            "\n",
            "1. **Architectural Masterpiece**: It is a prime example of Mughal architecture, combining elements from Persian, Islamic, and Indian styles. Its intricate design, use of precious stones, and symmetrical layout showcase the artistry of the time.\n",
            "\n",
            "2. **Symbol of Love**: Commissioned by Shah Jahan in memory of his beloved wife Mumtaz Mahal, the Taj Mahal is widely regarded as a symbol of eternal love and devotion.\n",
            "\n",
            "3. **UNESCO World Heritage Site**: Designated in 1983, it is recognized for its cultural significance and is considered one of the universally admired masterpieces of the world.\n",
            "\n",
            "4. **Tourist Attraction**: Attracting over five million visitors annually, it is one of the most popular tourist destinations in India, symbolizing the country's rich history and heritage.\n",
            "\n",
            "5. **Cultural Significance**: The Taj Mahal has become an iconic image associated with India, representing its history, culture, and architectural brilliance.\n",
            "\n",
            "6. **Symbolism and Alignment**: Its design incorporates elements of balance and harmony, with careful alignment to cardinal directions, enhancing its spiritual and symbolic significance.\n",
            "\n",
            "7. **Intricate Details**: The interior and exterior boast exquisite inlay work, calligraphy, and decorative features, highlighting the craftsmanship of Mughal artisans.\n",
            "\n",
            "These aspects collectively make the Taj Mahal not just a stunning structure, but also a profound symbol of love, artistry, and cultural heritage.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "ü§ñ Bot: It seems like your question got cut off. Could you please clarify what you would like to know about the Taj Mahal?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "üëã Goodbye!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# INTERACTIVE CHATBOT LOOP\n",
        "# ============================================================\n",
        "# This loop handles user interaction and command processing\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ TAJ MAHAL CHATBOT WITH MEMORY\")\n",
        "print(\"=\"*80)\n",
        "print(\"Ask me anything about the Taj Mahal!\")\n",
        "print(\"\\nAvailable commands:\")\n",
        "print(\"  ‚Ä¢ quit/exit  - End the conversation\")\n",
        "print(\"  ‚Ä¢ clear      - Reset conversation memory\")\n",
        "print(\"  ‚Ä¢ history    - View conversation history\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"üë§ You: \").strip()\n",
        "    \n",
        "    # ========================================\n",
        "    # COMMAND: Quit/Exit\n",
        "    # ========================================\n",
        "    if user_input.lower() in ['quit', 'exit']:\n",
        "        print(\"\\nüëã Goodbye! Thanks for chatting!\\n\")\n",
        "        break\n",
        "    \n",
        "    # ========================================\n",
        "    # COMMAND: Clear memory\n",
        "    # ========================================\n",
        "    elif user_input.lower() == 'clear':\n",
        "        clear_memory()\n",
        "        print(\"‚úÖ You can now start a fresh conversation.\\n\")\n",
        "        continue\n",
        "    \n",
        "    # ========================================\n",
        "    # COMMAND: Show history\n",
        "    # ========================================\n",
        "    elif user_input.lower() == 'history':\n",
        "        print(\"\\nüìú Conversation History:\")\n",
        "        print(\"-\"*80)\n",
        "        if conversation_history:\n",
        "            # Display all stored messages\n",
        "            for msg in conversation_history:\n",
        "                role_emoji = \"üë§\" if msg['role'] == 'user' else \"ü§ñ\"\n",
        "                print(f\"{role_emoji} {msg['role'].upper()}: {msg['message']}\\n\")\n",
        "        else:\n",
        "            print(\"No history yet.\\n\")\n",
        "        print(\"-\"*80 + \"\\n\")\n",
        "        continue\n",
        "    \n",
        "    # ========================================\n",
        "    # VALIDATION: Empty input\n",
        "    # ========================================\n",
        "    elif not user_input:\n",
        "        print(\"‚ö†Ô∏è  Please type something.\\n\")\n",
        "        continue\n",
        "    \n",
        "    # ========================================\n",
        "    # MAIN: Process question with RAG + Memory\n",
        "    # ========================================\n",
        "    try:\n",
        "        # Call the RAG function\n",
        "        answer = ask_question(user_input)\n",
        "        \n",
        "        # Display formatted response\n",
        "        print(f\"\\nü§ñ Bot: {answer}\\n\")\n",
        "        print(\"-\"*80 + \"\\n\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        # Handle any errors gracefully\n",
        "        print(f\"\\n‚ùå Error: {e}\\n\")\n",
        "        print(\"Please try asking your question differently.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 9: Testing the Chatbot\n",
        "\n",
        "### Recommended Test Questions\n",
        "\n",
        "Test the chatbot with these questions to see RAG + Memory in action:\n",
        "\n",
        "**1. Basic Retrieval (No Memory Required):**\n",
        "```\n",
        "Q: \"Who built the Taj Mahal?\"\n",
        "Expected: Bot searches vector DB and finds \"Shah Jahan\"\n",
        "```\n",
        "\n",
        "**2. Follow-up Using Memory:**\n",
        "```\n",
        "Q: \"Why did he build it?\"\n",
        "Expected: Bot uses memory to know \"he\" = Shah Jahan\n",
        "         Searches for motivation/reason\n",
        "```\n",
        "\n",
        "**3. New Topic (Fresh Retrieval):**\n",
        "```\n",
        "Q: \"What materials were used in construction?\"\n",
        "Expected: Bot searches for construction materials\n",
        "```\n",
        "\n",
        "**4. Follow-up with Reference:**\n",
        "```\n",
        "Q: \"Where did those materials come from?\"\n",
        "Expected: Bot remembers \"those materials\" from previous Q\n",
        "         Searches for material origins\n",
        "```\n",
        "\n",
        "**5. Complex Memory Test:**\n",
        "```\n",
        "Q: \"When was it built?\"\n",
        "Q: \"How long did the construction take?\"\n",
        "Q: \"Was it expensive?\" ‚Üê Requires remembering \"it\" = Taj Mahal\n",
        "```\n",
        "\n",
        "**6. Clear and Reset:**\n",
        "```\n",
        "Command: clear\n",
        "Q: \"Why was it built?\" ‚Üê Without memory, might need more context\n",
        "```\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "- **RAG in action**: See how vector search retrieves relevant chunks\n",
        "- **Memory persistence**: Notice how follow-up questions work seamlessly\n",
        "- **Context tracking**: Observe pronoun resolution (he, it, that, etc.)\n",
        "- **Sliding window**: Ask 6+ questions to see oldest memories fade\n",
        "- **Error handling**: Try edge cases and see graceful fallbacks\n",
        "\n",
        "### Pro Tips\n",
        "\n",
        "1. **Use `history` command** after a few exchanges to see memory structure\n",
        "2. **Test without `clear`** to see how context builds up\n",
        "3. **Ask ambiguous follow-ups** to test memory quality\n",
        "4. **Compare with/without memory** by clearing mid-conversation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary: What You've Built\n",
        "\n",
        "### Architecture Overview\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    USER QUESTION                            ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                      ‚îÇ\n",
        "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        ‚îÇ                           ‚îÇ\n",
        "        ‚ñº                           ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ   RETRIEVER   ‚îÇ          ‚îÇ     MEMORY     ‚îÇ\n",
        "‚îÇ  (LangChain)  ‚îÇ          ‚îÇ   (Custom)     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "        ‚îÇ                           ‚îÇ\n",
        "        ‚îÇ  Top 4 Chunks             ‚îÇ  Previous Q&A\n",
        "        ‚îÇ                           ‚îÇ\n",
        "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                  ‚îÇ\n",
        "                  ‚ñº\n",
        "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        ‚îÇ  Format Prompt  ‚îÇ\n",
        "        ‚îÇ  instructions + ‚îÇ\n",
        "        ‚îÇ     input       ‚îÇ\n",
        "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                 ‚îÇ\n",
        "                 ‚ñº\n",
        "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        ‚îÇ  OpenAI API     ‚îÇ\n",
        "        ‚îÇ responses.create‚îÇ\n",
        "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                 ‚îÇ\n",
        "                 ‚ñº\n",
        "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        ‚îÇ    RESPONSE     ‚îÇ\n",
        "        ‚îÇ  Update Memory  ‚îÇ\n",
        "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### Components Used\n",
        "\n",
        "**LangChain (Retrieval Layer):**\n",
        "- `WebBaseLoader`: Scrape web content\n",
        "- `RecursiveCharacterTextSplitter`: Chunk documents\n",
        "- `OpenAIEmbeddings`: Generate embeddings\n",
        "- `Chroma`: Vector database with persistence\n",
        "- `Retriever`: Semantic search interface\n",
        "\n",
        "**Direct OpenAI API (Generation Layer):**\n",
        "- `client.responses.create()`: Generate answers\n",
        "- `instructions`: System prompt + memory\n",
        "- `input`: Retrieved context + question\n",
        "- `output_text`: Clean string response\n",
        "\n",
        "**Custom Memory (Context Management):**\n",
        "- `conversation_history`: List of dicts\n",
        "- `add_to_memory()`: Save messages\n",
        "- `get_memory_text()`: Format for API\n",
        "- Sliding window: 10 messages max\n",
        "\n",
        "### Key Learnings\n",
        "\n",
        "1. **Hybrid Architecture**: Combine LangChain's strengths (retrieval) with direct API control (generation)\n",
        "2. **RAG Pipeline**: Retrieve ‚Üí Augment ‚Üí Generate workflow\n",
        "3. **Memory Management**: Manual tracking enables flexible conversation flow\n",
        "4. **Vector Search**: Semantic similarity finds relevant content, not just keywords\n",
        "5. **Prompt Engineering**: Separate context (input) from instructions (system prompt)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **Add more sources**: Scrape multiple URLs for broader knowledge\n",
        "- **Improve chunking**: Experiment with chunk size and overlap\n",
        "- **Better memory**: Implement summarization for longer conversations\n",
        "- **Add citations**: Return source documents with answers\n",
        "- **Deploy**: Build a web interface with Streamlit or Gradio"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.11.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
